{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "import joblib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"testing.log\"), logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "class CybercrimeClassifier:\n",
    "    def __init__(self, min_samples_per_class=2):\n",
    "        # Download required NLTK resources\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "        nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        # Add domain-specific stop words\n",
    "        self.stop_words.update(['please', 'help', 'thank', 'thanks', 'sir', 'madam', 'kindly'])\n",
    "        \n",
    "        self.label_encoders = {\n",
    "            \"category\": LabelEncoder(),\n",
    "            \"sub_category\": LabelEncoder(),\n",
    "        }\n",
    "        self.models = {\n",
    "            \"category\": None,\n",
    "            \"sub_category\": None\n",
    "        }\n",
    "        self.min_samples_per_class = min_samples_per_class\n",
    "        self.vectorizer_params = {\n",
    "            'max_features': 10000,  # Increased from 5000\n",
    "            'ngram_range': (1, 3),  # Added trigrams\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.95,\n",
    "            'analyzer': 'word',\n",
    "            'token_pattern': r'\\b\\w+\\b',  # Matches whole words\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced text preprocessing with domain-specific cleaning\"\"\"\n",
    "        try:\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove URLs\n",
    "            text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "            \n",
    "            # Remove email addresses\n",
    "            text = re.sub(r'\\S+@\\S+', '', text)\n",
    "            \n",
    "            # Remove phone numbers (various formats)\n",
    "            text = re.sub(r'\\+?\\d{10,}|\\+?\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4}', '', text)\n",
    "            \n",
    "            # Replace multiple spaces with single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            \n",
    "            # Keep important punctuation that might indicate sentiment or emphasis\n",
    "            text = re.sub(r'[^a-zA-Z\\s!?.]', '', text)\n",
    "\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            # Remove stopwords and lemmatize, keep tokens longer than 2 characters\n",
    "            tokens = [\n",
    "                self.lemmatizer.lemmatize(token)\n",
    "                for token in tokens\n",
    "                if token not in self.stop_words and len(token) > 2\n",
    "            ]\n",
    "\n",
    "            # Add bigrams and trigrams for important phrases\n",
    "            bigrams = [f\"{tokens[i]}_{tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "            trigrams = [f\"{tokens[i]}_{tokens[i+1]}_{tokens[i+2]}\" for i in range(len(tokens)-2)]\n",
    "            \n",
    "            processed_text = ' '.join(tokens + bigrams + trigrams)\n",
    "            \n",
    "            # Return empty string if processed text is too short\n",
    "            if len(processed_text.split()) < 3:\n",
    "                return \"\"\n",
    "                \n",
    "            return processed_text\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preprocessing text: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def build_model(self, class_labels: np.ndarray) -> Pipeline:\n",
    "        \"\"\"Create an improved classification pipeline with hyperparameter tuning\"\"\"\n",
    "        # Calculate class weights\n",
    "        n_samples = len(class_labels)\n",
    "        n_classes = len(np.unique(class_labels))\n",
    "        \n",
    "        # If severe class imbalance, adjust class weights\n",
    "        if n_classes > 1:\n",
    "            class_weights = dict(zip(\n",
    "                np.unique(class_labels),\n",
    "                n_samples / (n_classes * np.bincount(class_labels))\n",
    "            ))\n",
    "        else:\n",
    "            class_weights = None\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            (\"tfidf\", TfidfVectorizer(**self.vectorizer_params)),\n",
    "            (\"classifier\", RandomForestClassifier(\n",
    "                n_estimators=200,  # Increased from 100\n",
    "                max_depth=20,  # Added max_depth to prevent overfitting\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                class_weight=class_weights,\n",
    "                bootstrap=True\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        # Define parameters for grid search\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__max_depth': [10, 20, None],\n",
    "            'classifier__min_samples_split': [2, 5],\n",
    "            'tfidf__max_features': [5000, 10000],\n",
    "            'tfidf__ngram_range': [(1, 2), (1, 3)]\n",
    "        }\n",
    "\n",
    "        return GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='f1_weighted'\n",
    "        )\n",
    "\n",
    "    def analyze_class_distribution(self, df):\n",
    "        \"\"\"Analyze and print class distribution information\"\"\"\n",
    "        print(\"\\nClass Distribution Analysis:\", file=sys.stderr)\n",
    "        \n",
    "        for column in ['category', 'sub_category']:\n",
    "            counts = df[column].value_counts()\n",
    "            print(f\"\\n{column.upper()} Distribution:\", file=sys.stderr)\n",
    "            print(f\"Total unique classes: {len(counts)}\", file=sys.stderr)\n",
    "            print(f\"Classes with only one sample: {sum(counts == 1)}\", file=sys.stderr)\n",
    "            print(\"\\nTop 5 most common classes:\", file=sys.stderr)\n",
    "            print(counts.head().to_string(), file=sys.stderr)\n",
    "            print(\"\\nClasses with less than minimum samples:\", file=sys.stderr)\n",
    "            print(counts[counts < self.min_samples_per_class].to_string(), file=sys.stderr)\n",
    "        \n",
    "        return counts\n",
    "\n",
    "    def filter_rare_classes(self, df):\n",
    "        \"\"\"Filter out classes with too few samples\"\"\"\n",
    "        print(\"\\nFiltering rare classes...\", file=sys.stderr)\n",
    "        \n",
    "        original_len = len(df)\n",
    "        \n",
    "        # Filter both category and sub_category\n",
    "        for column in ['category', 'sub_category']:\n",
    "            counts = df[column].value_counts()\n",
    "            valid_classes = counts[counts >= self.min_samples_per_class].index\n",
    "            df = df[df[column].isin(valid_classes)]\n",
    "        \n",
    "        filtered_len = len(df)\n",
    "        print(f\"Removed {original_len - filtered_len} samples with rare classes\", file=sys.stderr)\n",
    "        print(f\"Remaining samples: {filtered_len}\", file=sys.stderr)\n",
    "        \n",
    "        if filtered_len == 0:\n",
    "            raise ValueError(\"No samples remaining after filtering rare classes. Consider lowering min_samples_per_class.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess the text data\"\"\"\n",
    "        try:\n",
    "            # Convert to string if not already\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            # Remove special characters and numbers\n",
    "            text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            # Remove stopwords and lemmatize\n",
    "            tokens = [\n",
    "                self.lemmatizer.lemmatize(token)\n",
    "                for token in tokens\n",
    "                if token not in self.stop_words and len(token) > 2\n",
    "            ]\n",
    "\n",
    "            return \" \".join(tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing text: {str(e)}\", file=sys.stderr)\n",
    "            return \"\"  # Return empty string in case of error\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare the data for training with additional validation\"\"\"\n",
    "        \n",
    "        # Add Unknown category for handling unseen labels\n",
    "        for column in ['category', 'sub_category']:\n",
    "            if 'Unknown' not in df[column].unique():\n",
    "                # Add a single example of Unknown category\n",
    "                unknown_row = df.iloc[0].copy()\n",
    "                unknown_row[column] = 'Unknown'\n",
    "                unknown_row['crimeaditionalinfo'] = 'unknown case'\n",
    "                df = pd.concat([df, pd.DataFrame([unknown_row])], ignore_index=True)\n",
    "        \n",
    "        # Analyze initial class distribution\n",
    "        self.analyze_class_distribution(df)\n",
    "        \n",
    "        # Filter rare classes\n",
    "        df = self.filter_rare_classes(df)\n",
    "        \n",
    "        # Analyze class distribution after filtering\n",
    "        print(\"\\nClass distribution after filtering:\", file=sys.stderr)\n",
    "        self.analyze_class_distribution(df)\n",
    "        \n",
    "        # Preprocess the text data\n",
    "        print(\"\\nPreprocessing text data...\", file=sys.stderr)\n",
    "        df['processed_text'] = [\n",
    "            self.preprocess_text(text) \n",
    "            for text in tqdm(df['crimeaditionalinfo'], desc=\"Preprocessing Text\")\n",
    "        ]\n",
    "        \n",
    "        # Remove empty processed texts\n",
    "        df = df[df['processed_text'].str.len() > 0]\n",
    "        print(f\"Samples after removing empty processed texts: {len(df)}\", file=sys.stderr)\n",
    "        \n",
    "        # Encode labels\n",
    "        print(\"\\nEncoding labels...\", file=sys.stderr)\n",
    "        for column in ['category', 'sub_category']:\n",
    "            df[f'{column}_encoded'] = self.label_encoders[column].fit_transform(df[column])\n",
    "            print(f\"Number of unique {column}s: {len(self.label_encoders[column].classes_)}\", file=sys.stderr)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def train(self, train_df: pd.DataFrame, test_df: pd.DataFrame = None) -> bool:\n",
    "        \"\"\"Enhanced training with cross-validation and error analysis\"\"\"\n",
    "        try:\n",
    "            # Prepare training data\n",
    "            prepared_train_df = self.prepare_data(train_df)\n",
    "            \n",
    "            if len(prepared_train_df) < self.min_samples_per_class * 2:\n",
    "                raise ValueError(f\"Not enough samples for training. Need at least {self.min_samples_per_class * 2} samples.\")\n",
    "            \n",
    "            # Split features for training\n",
    "            X = prepared_train_df['processed_text']\n",
    "            \n",
    "            # If no test set provided, create one\n",
    "            if test_df is None:\n",
    "                # Stratified split to maintain class distribution\n",
    "                X_train, X_val, y_train_dict, y_val_dict = {}, {}, {}, {}\n",
    "                \n",
    "                for column in ['category', 'sub_category']:\n",
    "                    y = prepared_train_df[f'{column}_encoded']\n",
    "                    X_train[column], X_val[column], y_train_dict[column], y_val_dict[column] = train_test_split(\n",
    "                        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "                    )\n",
    "            else:\n",
    "                prepared_test_df = self.prepare_data(test_df)\n",
    "                X_val = {\n",
    "                    'category': prepared_test_df['processed_text'],\n",
    "                    'sub_category': prepared_test_df['processed_text']\n",
    "                }\n",
    "                y_val_dict = {\n",
    "                    'category': prepared_test_df['category_encoded'],\n",
    "                    'sub_category': prepared_test_df['sub_category_encoded']\n",
    "                }\n",
    "                X_train = {'category': X, 'sub_category': X}\n",
    "                y_train_dict = {\n",
    "                    'category': prepared_train_df['category_encoded'],\n",
    "                    'sub_category': prepared_train_df['sub_category_encoded']\n",
    "                }\n",
    "\n",
    "            # Train and evaluate models for each target\n",
    "            for column in ['category', 'sub_category']:\n",
    "                print(f\"\\nTraining {column} model...\", file=sys.stderr)\n",
    "                \n",
    "                # Build and train the model with grid search\n",
    "                self.models[column] = self.build_model(y_train_dict[column])\n",
    "                self.models[column].fit(X_train[column], y_train_dict[column])\n",
    "                \n",
    "                # Print best parameters\n",
    "                print(f\"\\nBest parameters for {column}:\", file=sys.stderr)\n",
    "                print(self.models[column].best_params_, file=sys.stderr)\n",
    "                \n",
    "                # Make predictions and evaluate\n",
    "                y_pred = self.models[column].predict(X_val[column])\n",
    "                \n",
    "                # Print detailed evaluation metrics\n",
    "                print(f\"\\n{column.upper()} Classification Report:\", file=sys.stderr)\n",
    "                print(classification_report(\n",
    "                    y_val_dict[column],\n",
    "                    y_pred,\n",
    "                    target_names=self.label_encoders[column].classes_,\n",
    "                    zero_division=1\n",
    "                ))\n",
    "                \n",
    "                # Error analysis\n",
    "                self._perform_error_analysis(\n",
    "                    X_val[column],\n",
    "                    y_val_dict[column],\n",
    "                    y_pred,\n",
    "                    self.label_encoders[column].classes_,\n",
    "                    column\n",
    "                )\n",
    "            \n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _perform_error_analysis(self, X_val, y_true, y_pred, class_names, column):\n",
    "        \"\"\"Analyze prediction errors to identify patterns\"\"\"\n",
    "        # Convert encoded labels back to original names\n",
    "        y_true_names = [class_names[i] for i in y_true]\n",
    "        y_pred_names = [class_names[i] for i in y_pred]\n",
    "        \n",
    "        # Find misclassified examples\n",
    "        errors = [(true, pred, text) for true, pred, text in zip(y_true_names, y_pred_names, X_val) if true != pred]\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"\\nError Analysis for {column}:\", file=sys.stderr)\n",
    "            print(f\"Total errors: {len(errors)}\", file=sys.stderr)\n",
    "            \n",
    "            # Analyze common misclassifications\n",
    "            misclass_pairs = [(true, pred) for true, pred, _ in errors]\n",
    "            common_errors = pd.DataFrame(misclass_pairs, columns=['True', 'Predicted']).value_counts().head()\n",
    "            \n",
    "            print(\"\\nMost common misclassifications:\", file=sys.stderr)\n",
    "            print(common_errors.to_string(), file=sys.stderr)\n",
    "\n",
    "    def predict(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced prediction with confidence thresholds and error handling\"\"\"\n",
    "        try:\n",
    "            processed_text = self.preprocess_text(text)\n",
    "            if not processed_text:\n",
    "                return self._get_unknown_prediction()\n",
    "\n",
    "            results = {}\n",
    "            for column in ['category', 'sub_category']:\n",
    "                if self.models[column] is None:\n",
    "                    raise ValueError(f\"Model for {column} is not trained\")\n",
    "                \n",
    "                try:\n",
    "                    # Get prediction probabilities\n",
    "                    probas = self.models[column].predict_proba([processed_text])[0]\n",
    "                    max_proba = max(probas)\n",
    "                    prediction = self.models[column].predict([processed_text])[0]\n",
    "                    \n",
    "                    # Use confidence threshold\n",
    "                    if max_proba < 0.3:  # Adjusted confidence threshold\n",
    "                        results[column] = \"Unknown\"\n",
    "                        results[f\"{column}_confidence\"] = 0.0\n",
    "                    else:\n",
    "                        results[column] = self.label_encoders[column].classes_[prediction]\n",
    "                        results[f\"{column}_confidence\"] = float(max_proba)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error predicting {column}: {str(e)}\")\n",
    "                    results.update(self._get_unknown_prediction(column))\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during prediction: {str(e)}\")\n",
    "            return self._get_unknown_prediction()\n",
    "\n",
    "    def _get_unknown_prediction(self, column: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Helper method to return unknown prediction\"\"\"\n",
    "        if column:\n",
    "            return {\n",
    "                column: \"Unknown\",\n",
    "                f\"{column}_confidence\": 0.0\n",
    "            }\n",
    "        return {\n",
    "            \"category\": \"Unknown\",\n",
    "            \"category_confidence\": 0.0,\n",
    "            \"sub_category\": \"Unknown\",\n",
    "            \"sub_category_confidence\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "from cybercrime_classifier import CybercrimeClassifier\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatasetAnalyzer:\n",
    "    \"\"\"Analyzes dataset characteristics and quality.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_text_length(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze text length distribution.\"\"\"\n",
    "        text_lengths = df['crimeaditionalinfo'].str.len()\n",
    "        return {\n",
    "            'min_length': int(text_lengths.min()),\n",
    "            'max_length': int(text_lengths.max()),\n",
    "            'mean_length': float(text_lengths.mean()),\n",
    "            'median_length': float(text_lengths.median()),\n",
    "            'std_length': float(text_lengths.std())\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_class_distribution(df: pd.DataFrame) -> Dict[str, Dict[str, int]]:\n",
    "        \"\"\"Analyze class distribution for categories and sub-categories.\"\"\"\n",
    "        return {\n",
    "            'category': df['category'].value_counts().to_dict(),\n",
    "            'sub_category': df['sub_category'].value_counts().to_dict()\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_class_overlap(train_df: pd.DataFrame, test_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze class overlap between train and test sets.\"\"\"\n",
    "        train_categories = set(train_df['category'].unique())\n",
    "        test_categories = set(test_df['category'].unique())\n",
    "        train_subcategories = set(train_df['sub_category'].unique())\n",
    "        test_subcategories = set(test_df['sub_category'].unique())\n",
    "        \n",
    "        return {\n",
    "            'category': {\n",
    "                'train_only': list(train_categories - test_categories),\n",
    "                'test_only': list(test_categories - train_categories),\n",
    "                'common': list(train_categories & test_categories)\n",
    "            },\n",
    "            'sub_category': {\n",
    "                'train_only': list(train_subcategories - test_subcategories),\n",
    "                'test_only': list(test_subcategories - train_subcategories),\n",
    "                'common': list(train_subcategories & test_subcategories)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_potential_issues(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Detect potential data quality issues.\"\"\"\n",
    "        issues = {\n",
    "            'duplicate_texts': int(df['crimeaditionalinfo'].duplicated().sum()),\n",
    "            'very_short_texts': int((df['crimeaditionalinfo'].str.len() < 20).sum()),\n",
    "            'very_long_texts': int((df['crimeaditionalinfo'].str.len() > 1000).sum()),\n",
    "            'potential_noise': int((df['crimeaditionalinfo'].str.contains(r'[^\\w\\s.,!?@#$%&*()]')).sum())\n",
    "        }\n",
    "        return issues\n",
    "\n",
    "    def prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare and filter data, ensuring minimum samples per class.\"\"\"\n",
    "        try:\n",
    "            prepared_df = df.copy()\n",
    "            \n",
    "            # Preprocess text\n",
    "            prepared_df['processed_text'] = prepared_df['crimeaditionalinfo'].apply(self.preprocess_text)\n",
    "            \n",
    "            # Remove rows where preprocessing resulted in empty strings\n",
    "            prepared_df = prepared_df[prepared_df['processed_text'].str.len() > 0]\n",
    "            \n",
    "            # Filter out classes with insufficient samples\n",
    "            for column in ['category', 'sub_category']:\n",
    "                # Get class counts\n",
    "                class_counts = prepared_df[column].value_counts()\n",
    "                \n",
    "                # Get valid classes (those with enough samples)\n",
    "                valid_classes = class_counts[class_counts >= self.min_samples_per_class].index\n",
    "                \n",
    "                if len(valid_classes) == 0:\n",
    "                    raise ValueError(f\"No classes have the minimum required {self.min_samples_per_class} samples\")\n",
    "                \n",
    "                # Filter dataframe to keep only valid classes\n",
    "                prepared_df = prepared_df[prepared_df[column].isin(valid_classes)]\n",
    "                \n",
    "                # Fit label encoder on valid classes\n",
    "                self.label_encoders[column].fit(valid_classes)\n",
    "                \n",
    "                # Encode labels\n",
    "                prepared_df[f'{column}_encoded'] = self.label_encoders[column].transform(prepared_df[column])\n",
    "                \n",
    "                # Log removed classes\n",
    "                removed_classes = set(class_counts.index) - set(valid_classes)\n",
    "                if removed_classes:\n",
    "                    logging.warning(f\"Removed {len(removed_classes)} {column} classes with fewer than \"\n",
    "                                f\"{self.min_samples_per_class} samples: {removed_classes}\")\n",
    "            \n",
    "            return prepared_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preparing data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame, is_training: bool = True) -> Tuple[pd.DataFrame, list]:\n",
    "    \"\"\"Enhanced data cleaning with quality checks and reporting.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    cleaning_steps = [\n",
    "        ('Initial shape', lambda x: x, 'Initial dataset loaded'),\n",
    "        ('Remove empty texts', lambda x: x[x['crimeaditionalinfo'].str.strip() != ''], 'Removed empty texts'),\n",
    "        ('Fill missing values', lambda x: x.fillna({'category': 'Unknown', 'sub_category': 'Unknown', 'crimeaditionalinfo': ''}), 'Filled missing values'),\n",
    "        ('Convert to string', lambda x: x.assign(crimeaditionalinfo=x['crimeaditionalinfo'].astype(str)), 'Converted text to string'),\n",
    "        ('Clean special characters', lambda x: x.assign(\n",
    "            crimeaditionalinfo=x['crimeaditionalinfo'].str.replace(r'[^\\w\\s.,!?@#$%&*()]', ' ', regex=True)\n",
    "        ), 'Cleaned special characters')\n",
    "    ]\n",
    "    \n",
    "    # Only remove duplicates and very short texts from training data\n",
    "    if is_training:\n",
    "        cleaning_steps.extend([\n",
    "            ('Remove duplicates', lambda x: x.drop_duplicates(subset='crimeaditionalinfo'), 'Removed duplicate texts'),\n",
    "            ('Remove very short texts', lambda x: x[x['crimeaditionalinfo'].str.len() >= 20], 'Removed very short texts')\n",
    "        ])\n",
    "    \n",
    "    cleaning_report = []\n",
    "    \n",
    "    for step_name, step_func, step_desc in tqdm(cleaning_steps, desc=\"Cleaning Data\"):\n",
    "        initial_shape = df.shape[0]\n",
    "        df = step_func(df)\n",
    "        final_shape = df.shape[0]\n",
    "        \n",
    "        cleaning_report.append({\n",
    "            'step': step_name,\n",
    "            'description': step_desc,\n",
    "            'rows_before': initial_shape,\n",
    "            'rows_after': final_shape,\n",
    "            'rows_removed': initial_shape - final_shape\n",
    "        })\n",
    "    \n",
    "    return df, cleaning_report\n",
    "\n",
    "def save_metrics(metrics: Dict[str, Any], filename: str):\n",
    "    \"\"\"Save metrics with proper formatting.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "def train_and_save_model(\n",
    "    train_path: str = 'data/train.csv',\n",
    "    test_path: str = 'data/test.csv',\n",
    "    min_samples_per_class: int = 5,\n",
    "    output_dir: str = 'output'\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"Enhanced training pipeline with better error handling and class filtering.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        logger.info(\"Starting enhanced training pipeline...\")\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Load data\n",
    "        logger.info(\"Loading datasets...\")\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        # Analyze initial class distribution\n",
    "        logger.info(\"Analyzing class distribution...\")\n",
    "        for column in ['category', 'sub_category']:\n",
    "            class_counts = train_df[column].value_counts()\n",
    "            logger.info(f\"\\n{column} distribution:\")\n",
    "            logger.info(f\"Classes with 1 sample: {sum(class_counts == 1)}\")\n",
    "            logger.info(f\"Classes with 2-4 samples: {sum((class_counts >= 2) & (class_counts < 5))}\")\n",
    "            logger.info(f\"Classes with 5+ samples: {sum(class_counts >= 5)}\")\n",
    "        \n",
    "        # Clean data\n",
    "        train_df_cleaned, train_cleaning_report = clean_dataframe(train_df, is_training=True)\n",
    "        test_df_cleaned, test_cleaning_report = clean_dataframe(test_df, is_training=False)\n",
    "        \n",
    "        # Initialize classifier with minimum samples requirement\n",
    "        classifier = CybercrimeClassifier(min_samples_per_class=min_samples_per_class)\n",
    "        \n",
    "        # Prepare and filter data\n",
    "        logger.info(f\"Preparing data with minimum {min_samples_per_class} samples per class...\")\n",
    "        try:\n",
    "            train_df_filtered = classifier.prepare_data(train_df_cleaned)\n",
    "            test_df_filtered = classifier.prepare_data(test_df_cleaned)\n",
    "            \n",
    "            logger.info(f\"Training data shape after filtering: {train_df_filtered.shape}\")\n",
    "            logger.info(f\"Test data shape after filtering: {test_df_filtered.shape}\")\n",
    "            \n",
    "            # Create validation set\n",
    "            train_final, val_df = train_test_split(\n",
    "                train_df_filtered,\n",
    "                test_size=0.1,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            logger.info(\"Training model...\")\n",
    "            classifier.train(train_final, val_df)\n",
    "            \n",
    "            # Save model and metrics\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_filename = output_dir / f'cybercrime_classifier_{timestamp}.joblib'\n",
    "            metrics_filename = output_dir / f'training_metrics_{timestamp}.json'\n",
    "            \n",
    "            joblib.dump(classifier, model_filename)\n",
    "            \n",
    "            # Save summary metrics\n",
    "            metrics = {\n",
    "                'timestamp': timestamp,\n",
    "                'training_time': time.time() - start_time,\n",
    "                'data_shapes': {\n",
    "                    'initial_train': train_df.shape,\n",
    "                    'filtered_train': train_df_filtered.shape,\n",
    "                    'validation': val_df.shape,\n",
    "                    'test': test_df_filtered.shape\n",
    "                },\n",
    "                'class_counts': {\n",
    "                    'category': train_df_filtered['category'].value_counts().to_dict(),\n",
    "                    'sub_category': train_df_filtered['sub_category'].value_counts().to_dict()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(metrics_filename, 'w') as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "            \n",
    "            return str(model_filename), str(metrics_filename)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            logger.error(f\"Data preparation failed: {str(ve)}\")\n",
    "            raise\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        model_file, metrics_file = train_and_save_model()\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        print(f\"Model saved as: {model_file}\")\n",
    "        print(f\"Metrics saved as: {metrics_file}\")\n",
    "        \n",
    "        # Load and display key metrics\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "            print(\"\\nKey Performance Metrics:\")\n",
    "            print(f\"Training Time: {metrics['training_time']:.2f} seconds\")\n",
    "            print(\"\\nModel Performance:\")\n",
    "            print(json.dumps(metrics['model_performance'], indent=2))\n",
    "            \n",
    "            # Print class overlap warnings if any\n",
    "            overlap = metrics['data_analysis']['initial']['class_overlap']\n",
    "            if overlap['category']['test_only'] or overlap['sub_category']['test_only']:\n",
    "                print(\"\\nWarning: Test set contains classes not seen during training:\")\n",
    "                if overlap['category']['test_only']:\n",
    "                    print(f\"Categories: {', '.join(overlap['category']['test_only'])}\")\n",
    "                if overlap['sub_category']['test_only']:\n",
    "                    print(f\"Sub-categories: {', '.join(overlap['sub_category']['test_only'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
